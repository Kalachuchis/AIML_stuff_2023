{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: utf-9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, title)\n\u001b[0;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-9\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     55\u001b[0m         \u001b[39m# documents[title] = (f.read())\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(pattern, title):\n\u001b[0;32m     58\u001b[0m             spam\u001b[39m.\u001b[39mappend(f\u001b[39m.\u001b[39mread())\n",
      "File \u001b[1;32mc:\\Users\\ramon.galang\\anaconda3\\envs\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: utf-9"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize(string, method=\"word\"):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(\"(?:(?<=\\s)|(?<=^)|(?<=[>\\”]))[a-z-’]+(?:(?=\\s)|(?=\\:\\s)|(?=$)|(?=[.!,;\\”]))\")\n",
    "    # tokens = nltk.tokenize.word_tokenize(string.lower())\n",
    "    tokens = tokenizer.tokenize(string.lower())\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tags = [\n",
    "        (word, nltk.tag.map_tag(\"en-ptb\", \"universal\", tag))\n",
    "        for word, tag in tags\n",
    "    ] \n",
    "    words = [tag[0] for tag in tags if tag[1] not in ['.']]\n",
    "\n",
    "    return words\n",
    "    \n",
    "def generate_vocabulary(documents):\n",
    "    tokens = []\n",
    "    print(documents[0])\n",
    "    for doc in documents:\n",
    "        tokens.extend(tokenize(doc))\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    unique_words = list((dict(fdist).keys()))\n",
    "\n",
    "    vocabulary = \"\\n\".join(unique_words)\n",
    "\n",
    "    with open('training_vocab.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(vocabulary)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def generate_dcount_vectors(vocabulary, documents):\n",
    "    df = pd.DataFrame(vocabulary, columns=[\"Words\"])\n",
    "    return df\n",
    "\n",
    "def reduce_vocabulary(vocabulary, dcount_vectors, size):\n",
    "    pass\n",
    "\n",
    "documents = {}\n",
    "spam = []\n",
    "ham = []\n",
    "path = 'emails'\n",
    "\n",
    "pattern = 'spam'\n",
    "for title in os.listdir(path):\n",
    "    file = os.path.join(path, title)\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            # documents[title] = (f.read())\n",
    "\n",
    "            if re.search(pattern, title):\n",
    "                spam.append(f.read())\n",
    "            else:\n",
    "                ham.append(f.read())\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f)\n",
    "        print(\"file not found\")\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "documents['spam'] = spam\n",
    "documents['ham'] = ham\n",
    "all_docs = (np.array(list(documents.values())))\n",
    "all_docs = all_docs.flatten()\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('training_vocab.txt', 'r', encoding='utf-8') as f:\n",
    "        vocab = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    print(\"Vocabulary not found. Creating vocabulary, please wait...\")\n",
    "    generate_vocabulary(all_docs)\n",
    "    with open('training_vocab.txt','r', encoding='utf-8') as f:\n",
    "        vocab = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "generate_dcount_vectors(vocab, documents)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
